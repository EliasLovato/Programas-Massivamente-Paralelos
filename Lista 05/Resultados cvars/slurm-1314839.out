sdumont1243
sdumont1243
	linux-vdso.so.1 =>  (0x00007ffd991fe000)
	libmpifort.so.12 => /opt/intel/parallel_studio_xe_2020/compilers_and_libraries_2020.2.254/linux/mpi/intel64/lib/libmpifort.so.12 (0x00002b7360f4d000)
	libmpi.so.12 => /opt/intel/parallel_studio_xe_2020/compilers_and_libraries_2020.2.254/linux/mpi/intel64/lib/release/libmpi.so.12 (0x00002b736130c000)
	libdl.so.2 => /usr/lib64/libdl.so.2 (0x00002b7362528000)
	librt.so.1 => /usr/lib64/librt.so.1 (0x00002b736272c000)
	libpthread.so.0 => /usr/lib64/libpthread.so.0 (0x00002b7362934000)
	libm.so.6 => /usr/lib64/libm.so.6 (0x00002b7362b50000)
	libgcc_s.so.1 => /usr/lib64/libgcc_s.so.1 (0x00002b7362e52000)
	libc.so.6 => /usr/lib64/libc.so.6 (0x00002b7363068000)
	libfabric.so.1 => /opt/intel/parallel_studio_xe_2020/compilers_and_libraries_2020.2.254/linux/mpi/intel64/libfabric/lib/libfabric.so.1 (0x00002b7363435000)
	/lib64/ld-linux-x86-64.so.2 (0x00002b7360d29000)
615 MPI Control Variables
	I_MPI_DEBUG_OUTPUT              	@Default:# not defined
	I_MPI_DEBUG_COREDUMP            	@Default:# 1
	I_MPI_LIBRARY_KIND              	@Default:# not defined
	I_MPI_OFI_LIBRARY_INTERNAL      	@Default:# not defined
	I_MPI_CC_PROFILE                	@Default:# not defined
	I_MPI_CXX_PROFILE               	@Default:# not defined
	I_MPI_FC_PROFILE                	@Default:# not defined
	I_MPI_F77_PROFILE               	@Default:# not defined
	I_MPI_F90_PROFILE               	@Default:# not defined
	I_MPI_TRACE_PROFILE             	@Default:# not defined
	I_MPI_CHECK_PROFILE             	@Default:# not defined
	I_MPI_CHECK_COMPILER            	@Default:# not defined
	I_MPI_CC                        	@Default:# not defined
	I_MPI_CXX                       	@Default:# not defined
	I_MPI_FC                        	@Default:# not defined
	I_MPI_F90                       	@Default:# not defined
	I_MPI_F77                       	@Default:# not defined
	I_MPI_ROOT                      	@Default:# not defined
	I_MPI_ONEAPI_ROOT               	@Default:# not defined
	MPIR_CVAR_VT_ROOT               	@Default:# not defined
	I_MPI_COMPILER_CONFIG_DIR       	@Default:# not defined
	I_MPI_LINK                      	@Default:# not defined
	I_MPI_DEBUG_INFO_STRIP          	@Default:# not defined
	I_MPI_CFLAGS                    	@Default:# not defined
	I_MPI_CXXFLAGS                  	@Default:# not defined
	I_MPI_FCFLAGS                   	@Default:# not defined
	I_MPI_FFLAGS                    	@Default:# not defined
	I_MPI_LDFLAGS                   	@Default:# not defined
	I_MPI_FORT_BIND                 	@Default:# not defined
	I_MPI_AUTH_METHOD               	@Default:# not defined
	I_MPI_HYDRA_COLLECTIVE_LAUNCH   	@Default:# 1
	I_MPI_HYDRA_UNIQUE_PROXIES      	@Default:# not defined
	I_MPI_FAULT_CONTINUE            	@Default:# not defined
	I_MPI_FAULT_NODE_CONTINUE       	@Default:# not defined
	I_MPI_MPIRUN                    	@Default:# not defined
	I_MPI_BIND_ORDER                	@Default:# not defined
	I_MPI_BIND_NUMA                 	@Default:# not defined
	I_MPI_BIND_WIN_ALLOCATE         	@Default:# not defined
	I_MPI_HYDRA_NAMESERVER          	@Default:# not defined
	I_MPI_JOB_CHECK_LIBS            	@Default:# not defined
	I_MPI_HYDRA_SERVICE_PORT        	@Default:# not defined
	I_MPI_HYDRA_DEBUG               	@Default:# not defined
	I_MPI_HYDRA_ENV                 	@Default:# not defined
	I_MPI_JOB_TIMEOUT               	@Default:# not defined
	I_MPI_MPIEXEC_TIMEOUT           	@Default:# not defined
	I_MPI_JOB_STARTUP_TIMEOUT       	Set this environment variable to make mpiexec.hydra terminate$ the job in <timeout> seconds if some processes are not launched. @Default:# -1
	I_MPI_JOB_TIMEOUT_SIGNAL        	@Default:# not defined
	I_MPI_JOB_ABORT_SIGNAL          	@Default:# not defined
	I_MPI_JOB_SIGNAL_PROPAGATION    	@Default:# not defined
	I_MPI_HYDRA_BOOTSTRAP_EXEC      	@Default:# not defined
	I_MPI_HYDRA_BOOTSTRAP_EXEC_EXTRA_ARGS	@Default:# not defined
	I_MPI_HYDRA_BOOTSTRAP_AUTOFORK  	@Default:# not defined
	I_MPI_HYDRA_RMK                 	@Default:# not defined
	I_MPI_HYDRA_PMI_CONNECT         	@Default:# not defined
	I_MPI_HYDRA_TOPOLIB             	@Default:# not defined
	I_MPI_PORT_RANGE                	@Default:# not defined
	I_MPI_JOB_RESPECT_PROCESS_PLACEMENT	@Default:# not defined
	I_MPI_TMPDIR                    	@Default:# not defined
	I_MPI_HYDRA_DEMUX               	@Default:# not defined
	I_MPI_HYDRA_IFACE               	@Default:# not defined
	I_MPI_HYDRA_GDB_REMOTE_SHELL    	@Default:# not defined
	I_MPI_HYDRA_PMI_AGGREGATE       	@Default:# not defined
	I_MPI_JOB_TRACE_LIBS            	@Default:# not defined
	I_MPI_HYDRA_HOST_FILE           	Set the host file to run the application.$ Syntax$ I_MPI_HYDRA_HOST_FILE=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <hostsfile> - The full or relative path to the host file$ ----------------------------------------------------------------------- @Default:# not defined
	I_MPI_HYDRA_HOSTS_GROUP         	This environment variable allows to set node ranges using brackets,$ commas, and dashes (like in Slurm* Workload Manager). @Default:# not defined
	I_MPI_PERHOST                   	Define the default behavior for the -perhost option of the mpiexec.hydra$ command.$ Syntax$ I_MPI_PERHOST=<value>$ Arguments$ <value> - Define a value used for -perhost by default$ -----------------------------------------------------------------------$ <integer > 0> - Exact value for the option$ <all>         - All logical CPUs on the node$ <allcores>    - All cores (physical CPUs) on the node. This is the$  default value.$ ----------------------------------------------------------------------- @Default:# not defined
	I_MPI_GTOOL                     	Specify the tools to be launched for selected ranks. An alternative to$ this variable is the -gtool option$ Syntax$ I_MPI_GTOOL="<command line for a tool 1>:<ranks set 1>[=exclusive]$ [@arch 1];<command line for a tool 2>:<ranks set 2>[=exclusive]$ [@arch 2]; â€¦ ;<command line for a tool n>:<ranks set n>[=exclusive]$ [@arch n]"$ Arguments$ <arg> - Specify a tool launch command, including parameters.$ -----------------------------------------------------------------------$ <command line>     - Specify tool launch command, including parameters$ <rank set>         - Specify the range of ranks that are involved in the$  tool execution. Separate ranks with a comma or use the '-' symbol for$ a set ofcontiguous ranks. To run the tool forall ranks, use the all$  argument.$ [=exclusive]       - All cores (physical CPUs) on the node. This is$ the default value.$ [@arch]            - Specify the architecture on which the tool runs$  optional). For a given <rank set>, if you specify this argument,$ the tool is launched
	I_MPI_HYDRA_BRANCH_COUNT        	Set this environment variable to restrict the number of child management$ processes launched by the mpiexec.hydra operation or by each pmi_proxy$ anagement process.$ Syntax$ I_MPI_HYDRA_BRANCH_COUNT=<num>$ Arguments$ <value> - Number$ -----------------------------------------------------------------------$ <n> >= 0 - The default value is -1 if less than 128 nodes are used. $ This value also means that there is no hierarchical structure$ The default value is 32 if more than 127 nodes are used$ ----------------------------------------------------------------------- @Default:# not defined
	I_MPI_HYDRA_BOOTSTRAP           	Set this environment variable to specify the bootstrap server Syntax$ I_MPI_HYDRA_BOOTSTRAP=<arg>$ Arguments$ <arg> - String parameter$ -----------------------------------------------------------------------$ <ssh>     - Use secure shell. This is the default value$ <rsh>     - Use remote shell$ <pdsh>    - Use parallel distributed shell$ <pbsdsh>  - Use Torque* and PBS* pbsdsh command$ <fork>    - Use fork call$ <slurm>   - Use SLURM* srun command$ <ll>      - Use LoadLeveler* llspawn.stdio command$ <lsf>     - Use LSF* blaunch command$ <sge>     - Use Univa* Grid Engine* qrsh command$ ----------------------------------------------------------------------- @Default:# not defined
	I_MPI_PIN_UNIT                  	@Default:# not defined
	I_MPI_STATS                     	
	I_MPI_TIMER_ART                 	@Default:# 1
	MPIR_CVAR_INTEL_PIN             	Turn on/off process pinning.$ Syntax$ I_MPI_PIN=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable process pinning$ > disable | no | off | 0 - Disable processes pinning$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN @Default:# on
	MPIR_CVAR_INTEL_PIN_SHOW_REAL_MASK	Turn on/off real masks pinning print.$ Syntax$ I_MPI_PIN_SHOW_REAL_MASK=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable real pinning print$ > disable | no | off | 0 - Disable real pinning print$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_SHOW_REAL_MASK @Default:# on
	MPIR_CVAR_INTEL_PIN_PROCESSOR_LIST	Define a processor subset and the mapping rules for MPI processes within$ this subset.$ Syntax$ I_MPI_PIN_PROCESSOR_LIST=<value>$ The environment variable value has the following syntax forms:$ 1. <proclist>$ 2. [<procset>][:[grain=<grain>][,shift=<shift>]$ [,preoffset=<preoffset>][,postoffset=<postoffset>]$ 3. [<procset>][:map=<map>]$ @Alias:# I_MPI_PIN_PROCESSOR_LIST @Default:# not defined
	MPIR_CVAR_INTEL_PIN_PROCESSOR_EXCLUDE_LIST	Define a subset of logical processors to be excluded for the pinning$ capability on the intended hosts.$ Syntax$ I_MPI_PIN_PROCESSOR_EXCLUDE_LIST=<proclist>$ Arguments$ <proclist> - A comma-separated list of logical processor numbers$ and/or ranges of processors.$ -----------------------------------------------------------------------$ > <l> - Processor with logical number <l>.$ > <l>-<m> - Range of processors with logical numbers from <l> to <m>.$ > <k>,<l>-<m> - Processors <k>, as well as <l> through <m>.$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_PROCESSOR_EXCLUDE_LIST @Default:# not defined
	MPIR_CVAR_INTEL_PIN_CELL        	Set this environment variable to define the pinning resolution$ granularity. I_MPI_PIN_CELL specifies the minimal processor cell$ allocated when an MPI process is running.$ Syntax$ I_MPI_PIN_CELL=<cell>$ Arguments$ <cell> - Specify the resolution granularity$ -----------------------------------------------------------------------$ > unit - Basic processor unit (logical CPU)$ > core - Physical processor core$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_CELL @Default:# unit
	MPIR_CVAR_INTEL_PIN_RESPECT_CPUSET	Respect the process affinity mask.$ Syntax$ I_MPI_PIN_RESPECT_CPUSET=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Respect the process affinity mask$ > disable | no | off | 0 - Do not respect the process affinity mask$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_CPUSET @Default:# on
	MPIR_CVAR_INTEL_PIN_RESPECT_HCA 	In the presence of Intel(R) Omni-Path Architecture (Intel(R) OPA) or$ Infiniband architecture* host channel adapter (IBA* HCA),$ adjust the pinning according to the location of adapter.$ Syntax$ I_MPI_PIN_RESPECT_HCA=<value>$ Arguments$ <value> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 Use the location of IBA HCA if available$ > disable | no | off | 0 Do not use the location of IBA HCA$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_RESPECT_HCA @Default:# on
	MPIR_CVAR_INTEL_PIN_DOMAIN      	Intel(R) MPI Library provides environment variable to control process pinning for hybrid MPI/OpenMP* applications. This environment variable is used to define a number of non-overlapping subsets (domains) of logical processors on a node, and a set of rules on how MPI processes are bound to these domains by the following formula: one MPI process per one domain. Multi-core Shape:$ I_MPI_PIN_DOMAIN=<mc-shape>$ <mc-shape> - Define domains through multi-core terms.$ -----------------------------------------------------------------------$ > core - Each domain consists of the logical processors that share a$ particular core. The number of domains on a node is equal to the number$ of cores on the node.$ > socket | sock - Each domain consists of the logical processors that$ share a particular socket. The number of domains on a node is equal to$ the number of sockets on the node.$ > numa - Each domain consists of the logical processors that share a$ particular NUMA node. The number of domains on a machine is equal to$
	MPIR_CVAR_INTEL_PIN_ORDER       	Set this environment variable to define the mapping order for MPI$ processes to domains as specified by the$ I_MPI_PIN_DOMAIN environment variable.$ Syntax$ I_MPI_PIN_ORDER=<order>$ <order> - Specify the ranking order$ -----------------------------------------------------------------------$ > range - The domains are ordered according to the processor's BIOS$ numbering. This is a platform dependent numbering$ > scatter - The domains are ordered so that adjacent domains have$ minimal sharing of common resources$ > compact - The domains are ordered so that adjacent domains share$ common resources as much as possible. This is the default value$ > spread - The domains are ordered consecutively with the possibility$ not to share common resources$ > bunch - The processes are mapped proportionally to sockets and the$ domains are ordered as close as possible on the sockets$ ----------------------------------------------------------------------- @Alias:# I_MPI_PIN_ORDER @Default:# compact
	MPIR_CVAR_IMPI_HBW_POLICY       	@Alias:# I_MPI_HBW_POLICY
	MPIR_CVAR_IMPI_INTERNAL_MEM_POLICY	@Alias:# I_MPI_INTERNAL_MEM_POLICY
	MPIR_CVAR_IMPI_STATIC_BUILD     	@Alias:# I_MPI_STATIC_BUILD
	MPIR_CVAR_IMPI_RETURN_INTERNAL_MEM_NUMA	@Alias:# I_MPI_RETURN_INTERNAL_MEM_NUMA
	MPIR_CVAR_INTEL_EXTRA_FILESYSTEM	Turn on/off native parallel file systems support.$ Syntax$ I_MPI_EXTRA_FILESYSTEM=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > enable | yes | on | 1 - Enable native support for parallel file$ systems.$ > disable | no | off | 0 - Disable native support for parallel file$ systems.$ ----------------------------------------------------------------------- @Alias:# I_MPI_EXTRA_FILESYSTEM @Default:# off
	MPIR_CVAR_INTEL_EXTRA_FILESYSTEM_FORCE	Force filesystem recognition logic.$ Syntax$ I_MPI_EXTRA_FILESYSTEM_FORCE=<ufs|nfs|gpfs|panfs|lustre|daos>$ @Alias:# I_MPI_EXTRA_FILESYSTEM_FORCE @Default:# not defined
	MPIR_CVAR_INTEL_FABRICS         	Select the particular fabrics to be used.$ Syntax$ I_MPI_FABRICS=<ofi|shm:ofi>$ Arguments$ <fabric> -  Define a network fabric.$ -----------------------------------------------------------------------$ > shm - Shared memory transport (used for intra-node$ communication only).$ > ofi - OpenFabrics Interfaces* (OFI)-capable network fabrics, such as$ Intel(R) True Scale Fabric, Intel(R) Omni-Path Architecture, InfiniBand*$ and Ethernet (through OFI$ API).$ ----------------------------------------------------------------------- @Alias:# I_MPI_FABRICS @Default:# shm:ofi
	MPIR_CVAR_IMPI_MALLOC           	Enable or disable the Intel MPI private memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_MALLOC @Default:# 1
	MPIR_CVAR_INTEL_SHM_HEAP        	Enable or disable the Intel MPI shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP @Default:# -1
	MPIR_CVAR_INTEL_SHM_HEAP_OPT    	Shared memory heap optimization: "rank", "numa".$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_OPT @Default:# -1
	MPIR_CVAR_INTEL_SHM_HEAP_VSIZE  	Set shared memory heap virtual size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_VSIZE @Default:# -1
	MPIR_CVAR_INTEL_SHM_HEAP_CSIZE  	Set shared memory heap cache size (in MB).$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_CSIZE @Default:# -1
	MPIR_CVAR_INTEL_SHM_HEAP_NCONTIG_THRESHOLD	Set non-contig object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_NCONTIG_THRESHOLD @Default:# -1
	MPIR_CVAR_INTEL_SHM_HEAP_THRESHOLD	Set object size threshold for use shared memory allocator.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_HEAP_THRESHOLD @Default:# -1
	MPIR_CVAR_INTEL_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD	Threshold for short size messages receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_SHORT_MEMCPY_THRESHOLD @Default:# -1
	MPIR_CVAR_INTEL_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD	Threshold for regular size message receive via SHM HEAP transport.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_HEAP_REGULAR_MEMCPY_THRESHOLD @Default:# -1
	MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_SHORT_MEMCPY	Name of memory copy function for short message receive via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_SHORT_MEMCPY @Default:# ""
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY	Name of memory copy function for short message receive via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_SHORT_MEMCPY @Default:# ""
	MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY	Name of memory copy function for regular receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_REGULAR_MEMCPY @Default:# ""
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY	Name of memory copy function for regular receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_REGULAR_MEMCPY @Default:# ""
	MPIR_CVAR_INTEL_SHM_RECV_INTER_HEAP_MEMCPY	Name of memory copy function for receive messages via SHM HEAP transport, inter socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTER_HEAP_MEMCPY @Default:# ""
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_HEAP_MEMCPY	Name of memory copy function for receive messages via SHM HEAP transport, intra socket.$ ----------------------------------------------------------------------- @Alias:# I_MPI_SHM_RECV_INTRA_HEAP_MEMCPY @Default:# ""
	MPIR_CVAR_CH4_SHM_POSIX_EAGER   	Select a shared memory transport to be used.$ Syntax$ I_MPI_SHM=<transport>$ Arguments$ <transport> - Define a shared memory transport solution.$ -----------------------------------------------------------------------$ > disable | no | off | 0 - Do not use shared memory transport.$ > auto - Select a shared memory transport solution automatically.$ > bdw_sse - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The SSE/SSE2/SSE3 instruction$ set is used.$ > bdw_avx2 - The shared memory transport solution tuned for Intel(R)$ microarchitecture code name Broadwell. The AVX2 instruction set is used.$ > skx_sse - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The SSE/SSE2/SSE3 instruction set is used.$ > skx_avx2 - The shared memory transport solution tuned for Intel(R)$ Xeon(R) processors based on Intel(R) microarchitecture code name Skylake.$ The AVX2 instruction set is used.$ > skx_av
	MPIR_CVAR_INTEL_SHM_OPT         	Select a shared memory transport optimization strategy to be used.$ Syntax$ I_MPI_SHM_OPT=<optimization_strategy>$ Arguments$ <optimization_strategy> - Define a shared memory transport optimization strategy.$ -----------------------------------------------------------------------$ > dynamic - Let shared memory transport make decision in runtime.$ > intra - Optimize intra socket message passing.$ > inter - Optimize inter socket message passing.$ -----------------------------------------------------------------------$ @Alias:# I_MPI_SHM_OPT @Default:# dynamic
	MPIR_CVAR_INTEL_SHM_CELL_FWD_SIZE	Change the size of a shared memory forward cell. @Alias:# I_MPI_SHM_CELL_FWD_SIZE @Default:# -1
	MPIR_CVAR_INTEL_SHM_CELL_BWD_SIZE	Change the size of a shared memory backward cell. @Alias:# I_MPI_SHM_CELL_BWD_SIZE @Default:# -1
	MPIR_CVAR_INTEL_SHM_CELL_EXT_SIZE	Change the size of a shared memory extended cell. @Alias:# I_MPI_SHM_CELL_EXT_SIZE @Default:# -1
	MPIR_CVAR_INTEL_SHM_CELL_FWD_NUM	Change the number of forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_NUM @Default:# -1
	MPIR_CVAR_INTEL_SHM_CELL_FWD_HOLD_NUM	Change the number of hold forward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_FWD_HOLD_NUM @Default:# -1
	MPIR_CVAR_INTEL_SHM_CELL_BWD_NUM	Change the number of backward cells in the shared memory transport (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUM @Default:# -1
	MPIR_CVAR_INTEL_SHM_CELL_BWD_NUMA_AWARE	Use NUMA aware backward cells (1 : true, 0 : false, -1 : auto) (per rank). @Alias:# I_MPI_SHM_CELL_BWD_NUMA_AWARE @Default:# -1
	MPIR_CVAR_INTEL_SHM_CELL_EXT_NUM_TOTAL	Change the total number of extended cells in the shared memory$ transport. @Alias:# I_MPI_SHM_CELL_EXT_NUM_TOTAL @Default:# -1
	MPIR_CVAR_INTEL_SHM_MCDRAM_LIMIT	Change the size of the shared memory bound to the multi-channel DRAM (MCDRAM) (size per rank). @Alias:# I_MPI_SHM_MCDRAM_LIMIT @Default:# -1
	MPIR_CVAR_INTEL_SHM_SEND_SPIN_COUNT	Control the spin count value for the shared memory transport for sending messages. @Alias:# I_MPI_SHM_SEND_SPIN_COUNT @Default:# -1
	MPIR_CVAR_INTEL_SHM_RECV_SPIN_COUNT	Control the spin count value for the shared memory transport for receiving messages. @Alias:# I_MPI_SHM_RECV_SPIN_COUNT @Default:# -1
	MPIR_CVAR_INTEL_SHM_FILE_PREFIX_4K	Mount point of 4K page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_4K @Default:# ""
	MPIR_CVAR_INTEL_SHM_FILE_PREFIX_2M	Mount point of 2M huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_2M @Default:# ""
	MPIR_CVAR_INTEL_SHM_FILE_PREFIX_1G	Mount point of 1G huge page size shm files. @Alias:# I_MPI_SHM_FILE_PREFIX_1G @Default:# ""
	MPIR_CVAR_INTEL_SHM_PAUSE       	The number of pauses repeated. @Alias:# I_MPI_SHM_PAUSE
	MPIR_CVAR_INTEL_SHM_EAGER_THRESHOLD	Eager threshold. @Alias:# I_MPI_SHM_EAGER_THRESHOLD
	MPIR_CVAR_INTEL_SHM_RING_SIZE   	@Alias:# I_MPI_SHM_RING_SIZE
	MPIR_CVAR_INTEL_SHM_RING_ACK_THRESHOLD	@Alias:# I_MPI_SHM_RING_ACK_THRESHOLD
	MPIR_CVAR_INTEL_SHM_CELL_FWD_FIRST	@Alias:# I_MPI_SHM_CELL_FWD_FIRST
	MPIR_CVAR_INTEL_SHM_PROFILER_DIRECTORY	@Alias:# I_MPI_SHM_PROFILER_DIRECTORY
	MPIR_CVAR_INTEL_SHM_TRACE_DIRECTORY	@Alias:# I_MPI_SHM_TRACE_DIRECTORY
	MPIR_CVAR_INTEL_SHM_FRAME_SIZE  	@Alias:# I_MPI_SHM_FRAME_SIZE
	MPIR_CVAR_INTEL_SHM_FRAME_THRESHOLD	@Alias:# I_MPI_SHM_FRAME_THRESHOLD
	MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY	@Alias:# I_MPI_SHM_SEND_TINY_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_RING_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTRA_RING_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTER_RING_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTER_RING_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_RING_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTRA_RING_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTER_RING_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTER_RING_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_FWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTRA_CELL_FWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_FWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTER_CELL_FWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_CELL_BWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTRA_CELL_BWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTER_CELL_BWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTER_CELL_BWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTRA_CELL_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTER_CELL_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTRA_CELL_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTER_CELL_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTER_CELL_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTRA_FRAME_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTER_FRAME_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTRA_FRAME_FWD_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTER_FRAME_FWD_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTRA_FRAME_BWD_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_SEND_INTER_FRAME_BWD_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTRA_FRAME_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY	@Alias:# I_MPI_SHM_RECV_INTER_FRAME_REGULAR_MEMCPY
	MPIR_CVAR_INTEL_SHM_INPLACE_THRESHOLD	@Alias:# I_MPI_SHM_INPLACE_THRESHOLD
	MPIR_CVAR_INTEL_SHM_SEND_TINY_MEMCPY_THRESHOLD	@Alias:# I_MPI_SHM_SEND_TINY_MEMCPY_THRESHOLD
	MPIR_CVAR_INTEL_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD	@Alias:# I_MPI_SHM_RECV_CELL_REGULAR_MEMCPY_THRESHOLD
	MPIR_CVAR_INTEL_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD	@Alias:# I_MPI_SHM_SEND_INTER_UNIDIR_REGULAR_MEMCPY_THRESHOLD
	MPIR_CVAR_INTEL_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD	@Alias:# I_MPI_SHM_SEND_INTER_BIDIR_REGULAR_MEMCPY_THRESHOLD
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN	@Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MIN
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX	@Alias:# I_MPI_SHM_SEND_INTRA_UNIDIR_REGULAR_MEMCPY_THRESHOLD_MAX
	MPIR_CVAR_INTEL_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD	@Alias:# I_MPI_SHM_SEND_INTRA_BIDIR_REGULAR_MEMCPY_THRESHOLD
	MPIR_CVAR_INTEL_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD	@Alias:# I_MPI_SHM_RECV_FRAME_REGULAR_MEMCPY_THRESHOLD
	MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY	@Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTRA_CAPACITY
	MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY	@Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_REGULAR_CAPACITY
	MPIR_CVAR_INTEL_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY	@Alias:# I_MPI_SHM_CELL_FWD_UNIDIR_INTER_NONTEMPORAL_CAPACITY
	MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY	@Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTRA_CAPACITY
	MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY	@Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_REGULAR_CAPACITY
	MPIR_CVAR_INTEL_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY	@Alias:# I_MPI_SHM_CELL_FWD_BIDIR_INTER_NONTEMPORAL_CAPACITY
	MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN	@Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MIN
	MPIR_CVAR_INTEL_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX	@Alias:# I_MPI_SHM_SEND_FWD_FRAME_NONTEMPORAL_MEMCPY_THRESHOLD_MAX
	MPIR_CVAR_INTEL_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD	@Alias:# I_MPI_SHM_SEND_FWD_CELL_NONTEMPORAL_MEMCPY_THRESHOLD
	MPIR_CVAR_INTEL_SHM_HEAP_THP    	@Alias:# I_MPI_SHM_HEAP_THP
	MPIR_CVAR_INTEL_SHM_THP         	@Alias:# I_MPI_SHM_THP
	MPIR_CVAR_OFI_USE_PROVIDER      	Define the name of the OFI provider to load.$ Syntax$ I_MPI_OFI_PROVIDER=<name>$ Arguments$ <name> - The name of the OFI provider to load @Alias:# I_MPI_OFI_PROVIDER @Default:# not defined
	MPIR_CVAR_OFI_DUMP_PROVIDERS    	Control the capability of printing information about all OFI providers$ and their attributes from an OFI library.$ Syntax$ I_MPI_OFI_PROVIDER_DUMP=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > yes | on | 1 - Print the list of all OFI providers and their$ attributes from an OFI library$ > no | off | 0 - No action$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_PROVIDER_DUMP @Default:# off
	MPIR_CVAR_CH4_OFI_ENABLE_DIRECT_RECV	Control the capability of the direct receive in the OFI fabric.$ Syntax$ I_MPI_OFI_DRECV=<arg>$ Arguments$ <arg> - Binary indicator$ -----------------------------------------------------------------------$ > 1 - Enable direct receive$ > 0 - Disable direct receive$ ----------------------------------------------------------------------- @Alias:# I_MPI_OFI_DRECV @Default:# not defined
	MPIR_CVAR_OFI_MAX_MSG_SIZE      	@Alias:# I_MPI_OFI_MAX_MSG_SIZE
	MPIR_CVAR_OFI_LMT_WIN_SIZE      	@Alias:# I_MPI_OFI_LMT_WIN_SIZE
	MPIR_CVAR_CH4_OFI_ISEND_INJECT_THRESHOLD	@Alias:# I_MPI_OFI_ISEND_INJECT_THRESHOLD
	MPIR_CVAR_CH4_OFI_ADDRESS_EXCHANGE_MODE	@Alias:# I_MPI_STARTUP_MODE
	MPIR_CVAR_CH4_OFI_LARGE_SCALE_THRESHOLD	@Alias:# I_MPI_LARGE_SCALE_THRESHOLD
	MPIR_CVAR_CH4_OFI_EXTREME_SCALE_THRESHOLD	@Alias:# I_MPI_EXTREME_SCALE_THRESHOLD
	MPIR_CVAR_CH4_OFI_DYNAMIC_CONNECTION	@Alias:# I_MPI_DYNAMIC_CONNECTION
	MPIR_CVAR_CH4_OFI_EXPERIMENTAL  	@Alias:# I_MPI_OFI_EXPERIMENTAL @Default:# 0
	MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG	Prints out the configuration of each capability selected via the capability sets interface.
	MPIR_CVAR_CH4_OFI_ENABLE_DATA   	Enable immediate data fields in OFI to transmit source rank outside of the match bits
	MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE	If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
	MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS	If true, use OFI scalable endpoints.
	MPIR_CVAR_CH4_OFI_MAX_ENDPOINTS 	Specifies the maximum number of OFI endpoints that can be used by the OFI provider. The default value is -1, indicating that no value is set.
	MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE	If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
	MPIR_CVAR_CH4_OFI_ENABLE_TAGGED 	If true, use tagged message transmission functions in OFI.
	MPIR_CVAR_CH4_OFI_ENABLE_AM     	If true, enable OFI active message support.
	MPIR_CVAR_CH4_OFI_ENABLE_RMA    	If true, enable OFI RMA support for MPI RMA operations. OFI support for basic RMA is always required to implement large messgage transfers in the active message code path.
	MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS	If true, enable OFI Atomics support.
	MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS	Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
	MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS	If true, enable MPI data auto progress.
	MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS	If true, enable MPI control auto progress.
	MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK	If true, enable iovec for pt2pt.
	MPIR_CVAR_CH4_OFI_RANK_BITS     	Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
	MPIR_CVAR_CH4_OFI_TAG_BITS      	Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
	MPIR_CVAR_CH4_OFI_MAJOR_VERSION 	Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
	MPIR_CVAR_CH4_OFI_MINOR_VERSION 	Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
	MPIR_CVAR_CH4_OFI_ZERO_OP_FLAGS 	Zeroes rx_attr.op_flags and tx_attr.op_flags, disables use of FI_SELECTIVE_COMPLETION. Can give more optimized behavior of underlying provider.
	MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS	Specifies the number of buffers for receiving active messages.
	MPIR_CVAR_INTEL_COLL_DIRECT_PROGRESS	@Alias:# I_MPI_COLL_DIRECT_PROGRESS
	MPIR_CVAR_ENABLE_HCOLL          	@Alias:# I_MPI_COLL_EXTERNAL
	MPIR_CVAR_USE_ALLGATHER         	Control selection of MPI_Allgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLGATHER @Default:# -1
	MPIR_CVAR_USE_ALLGATHER_LIST    	@Alias:# I_MPI_ADJUST_ALLGATHER_LIST @Default:# -1
	MPIR_CVAR_USE_ALLGATHER_COMPOSITION	@Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION @Default:# -1
	MPIR_CVAR_ALLGATHER_COMPOSITION_DELTA_SEGSIZE	@Alias:# I_MPI_ADJUST_ALLGATHER_COMPOSITION_DELTA_SEGSIZE @Default:# -1
	MPIR_CVAR_USE_ALLGATHER_NETWORK 	@Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK @Default:# -1
	MPIR_CVAR_USE_ALLGATHER_NODE    	@Alias:# I_MPI_ADJUST_ALLGATHER_NODE @Default:# -1
	MPIR_CVAR_USE_ALLGATHERV        	Control selection of MPI_Allgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_ALLGATHERV @Default:# -1
	MPIR_CVAR_USE_ALLGATHERV_LIST   	@Alias:# I_MPI_ADJUST_ALLGATHERV_LIST @Default:# -1
	MPIR_CVAR_USE_ALLGATHERV_COMPOSITION	@Alias:# I_MPI_ADJUST_ALLGATHERV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ALLGATHERV_NETWORK	@Alias:# I_MPI_ADJUST_ALLGATHERV_NETWORK @Default:# -1
	MPIR_CVAR_USE_ALLGATHERV_NODE   	@Alias:# I_MPI_ADJUST_ALLGATHERV_NODE @Default:# -1
	MPIR_CVAR_ALLGATHER_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_ALLGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_ALLGATHER_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_ALLGATHER_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_SCATTERV_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_SCATTERV_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_SCATTERV_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_SCATTERV_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_SCATTER_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_SCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_SCATTER_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_SCATTER_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_USE_ALLREDUCE         	Control selection of MPI_Allreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-24 @Alias:# I_MPI_ADJUST_ALLREDUCE @Default:# -1
	MPIR_CVAR_USE_ALLREDUCE_LIST    	@Alias:# I_MPI_ADJUST_ALLREDUCE_LIST @Default:# -1
	MPIR_CVAR_USE_ALLREDUCE_COMPOSITION	@Alias:# I_MPI_ADJUST_ALLREDUCE_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ALLREDUCE_NETWORK 	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK @Default:# -1
	MPIR_CVAR_USE_ALLREDUCE_NODE    	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE @Default:# -1
	MPIR_CVAR_ALLREDUCE_NETWORK_MULTIPLYING_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_MULTIPLYING_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_MULTIPLYING_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_MULTIPLYING_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NETWORK_KARY_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_KARY_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_KARY_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_KARY_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_TYPE	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_SHM_GATHER_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_TYPE	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_SHM_RELEASE_RADIX	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_SHUMILIN_SEGSIZE	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_SHUMILIN_SEGSIZE @Default:# -1
	MPIR_CVAR_ALLREDUCE_ZETA_RADIX  	@Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_RADIX @Default:# -1
	MPIR_CVAR_ALLREDUCE_ZETA_SHM_TYPE	@Alias:# I_MPI_ADJUST_ALLREDUCE_ZETA_SHM_TYPE @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
	MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_NB_ALLTOALL @Default:# -1
	MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_NB_ALLTOALL @Default:# -1
	MPIR_CVAR_ALLREDUCE_NODE_NREDUCE_SEGSIZE	@Alias:# I_MPI_ADJUST_ALLREDUCE_NODE_NREDUCE_SEGSIZE @Default:# -1
	MPIR_CVAR_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE	@Alias:# I_MPI_ADJUST_ALLREDUCE_NETWORK_NREDUCE_SEGSIZE @Default:# -1
	MPIR_CVAR_USE_ALLTOALL          	Control selection of MPI_Alltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_ALLTOALL @Default:# -1
	MPIR_CVAR_USE_ALLTOALL_LIST     	@Alias:# I_MPI_ADJUST_ALLTOALL_LIST @Default:# -1
	MPIR_CVAR_USE_ALLTOALL_COMPOSITION	@Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ALLTOALL_NETWORK  	@Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK @Default:# -1
	MPIR_CVAR_USE_ALLTOALL_NODE     	@Alias:# I_MPI_ADJUST_ALLTOALL_NODE @Default:# -1
	MPIR_CVAR_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE	@Alias:# I_MPI_ADJUST_ALLTOALL_COMPOSITION_GAMMA_SEGSIZE @Default:# -1
	MPIR_CVAR_ALLTOALL_THROTTLE     	@Alias:# I_MPI_ADJUST_ALLTOALL_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_ALLTOALL_NETWORK_SCATTERED_THROTTLE	@Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_ALLTOALL_NODE_SCATTERED_THROTTLE	@Alias:# I_MPI_ADJUST_ALLTOALL_NODE_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_ALLTOALL_BRUCKS_RADIX 	@Alias:# I_MPI_ADJUST_ALLTOALL_BRUCKS_RADIX @Default:# -1
	MPIR_CVAR_ALLTOALL_NETWORK_BRUCKS_RADIX	@Alias:# I_MPI_ADJUST_ALLTOALL_NETWORK_BRUCKS_RADIX @Default:# -1
	MPIR_CVAR_ALLTOALL_NODE_BRUCKS_RADIX	@Alias:# I_MPI_ADJUST_ALLTOALL_NODE_BRUCKS_RADIX @Default:# -1
	MPIR_CVAR_USE_ALLTOALLV         	Control selection of MPI_Alltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_ALLTOALLV @Default:# -1
	MPIR_CVAR_USE_ALLTOALLV_LIST    	@Alias:# I_MPI_ADJUST_ALLTOALLV_LIST @Default:# -1
	MPIR_CVAR_USE_ALLTOALLV_COMPOSITION	@Alias:# I_MPI_ADJUST_ALLTOALLV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ALLTOALLV_NETWORK 	@Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK @Default:# -1
	MPIR_CVAR_USE_ALLTOALLV_NODE    	@Alias:# I_MPI_ADJUST_ALLTOALLV_NODE @Default:# -1
	MPIR_CVAR_ALLTOALLV_THROTTLE    	@Alias:# I_MPI_ADJUST_ALLTOALLV_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_ALLTOALLV_NETWORK_SCATTERED_THROTTLE	@Alias:# I_MPI_ADJUST_ALLTOALLV_NETWORK_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_ALLTOALLV_NODE_SCATTERED_THROTTLE	@Alias:# I_MPI_ADJUST_ALLTOALLV_NODE_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_USE_ALLTOALLW         	Control selection of MPI_Alltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_ALLTOALLW @Default:# -1
	MPIR_CVAR_USE_ALLTOALLW_LIST    	@Alias:# I_MPI_ADJUST_ALLTOALLW_LIST @Default:# -1
	MPIR_CVAR_USE_ALLTOALLW_COMPOSITION	@Alias:# I_MPI_ADJUST_ALLTOALLW_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ALLTOALLW_NETWORK 	@Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK @Default:# -1
	MPIR_CVAR_USE_ALLTOALLW_NODE    	@Alias:# I_MPI_ADJUST_ALLTOALLW_NODE @Default:# -1
	MPIR_CVAR_ALLTOALLW_THROTTLE    	@Alias:# I_MPI_ADJUST_ALLTOALLW_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_ALLTOALLW_NETWORK_SCATTERED_THROTTLE	@Alias:# I_MPI_ADJUST_ALLTOALLW_NETWORK_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_ALLTOALLW_NODE_SCATTERED_THROTTLE	@Alias:# I_MPI_ADJUST_ALLTOALLW_NODE_SCATTERED_THROTTLE @Default:# -1
	MPIR_CVAR_USE_BARRIER           	Control selection of MPI_Barrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-11 @Alias:# I_MPI_ADJUST_BARRIER @Default:# -1
	MPIR_CVAR_USE_BARRIER_LIST      	@Alias:# I_MPI_ADJUST_BARRIER_LIST @Default:# -1
	MPIR_CVAR_USE_BARRIER_COMPOSITION	@Alias:# I_MPI_ADJUST_BARRIER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_BARRIER_NETWORK   	@Alias:# I_MPI_ADJUST_BARRIER_NETWORK @Default:# -1
	MPIR_CVAR_USE_BARRIER_NODE      	@Alias:# I_MPI_ADJUST_BARRIER_NODE @Default:# -1
	MPIR_CVAR_BARRIER_NETWORK_MULTIPLYING_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NETWORK_MULTIPLYING_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NETWORK_KARY_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NETWORK_KARY_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NETWORK_RECURSIVE_EXCHANGE_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NODE_KARY_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NODE_KARY_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NODE_RECURSIVE_EXCHANGE_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NODE_SHM_GATHER_TYPE	@Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_TYPE @Default:# -1
	MPIR_CVAR_BARRIER_NODE_SHM_GATHER_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_GATHER_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_TYPE	@Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_TYPE @Default:# -1
	MPIR_CVAR_BARRIER_NODE_SHM_RELEASE_RADIX	@Alias:# I_MPI_ADJUST_BARRIER_NODE_SHM_RELEASE_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_ZETA_RADIX    	@Alias:# I_MPI_ADJUST_BARRIER_ZETA_RADIX @Default:# -1
	MPIR_CVAR_BARRIER_ZETA_SHM_TYPE 	@Alias:# I_MPI_ADJUST_BARRIER_ZETA_SHM_TYPE @Default:# -1
	MPIR_CVAR_USE_BCAST             	Control selection of MPI_Bcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-17 @Alias:# I_MPI_ADJUST_BCAST @Default:# -1
	MPIR_CVAR_USE_BCAST_LIST        	@Alias:# I_MPI_ADJUST_BCAST_LIST @Default:# -1
	MPIR_CVAR_USE_BCAST_COMPOSITION 	@Alias:# I_MPI_ADJUST_BCAST_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_BCAST_NETWORK     	@Alias:# I_MPI_ADJUST_BCAST_NETWORK @Default:# -1
	MPIR_CVAR_USE_BCAST_NODE        	@Alias:# I_MPI_ADJUST_BCAST_NODE @Default:# -1
	MPIR_CVAR_BCAST_EPSILON_NRAIL   	@Alias:# I_MPI_ADJUST_BCAST_EPSILON_NRAIL @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_KARY_RADIX	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_RADIX @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_KARY_SEGSIZE	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_KARY_SEGSIZE @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_KNOMIAL_SEGSIZE	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
	MPIR_CVAR_BCAST_NODE_KARY_RADIX 	@Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_RADIX @Default:# -1
	MPIR_CVAR_BCAST_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_BCAST_NODE_KARY_SEGSIZE	@Alias:# I_MPI_ADJUST_BCAST_NODE_KARY_SEGSIZE @Default:# -1
	MPIR_CVAR_BCAST_NODE_KNOMIAL_SEGSIZE	@Alias:# I_MPI_ADJUST_BCAST_NODE_KNOMIAL_SEGSIZE @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_TREE_RADIX	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_RADIX @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_TREE_SEGSIZE	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_SEGSIZE @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_TREE_TYPE	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_TYPE @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_TREE_THROTTLE	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_TREE_THROTTLE @Default:# -1
	MPIR_CVAR_BCAST_NODE_SHUMILIN_SEGSIZE	@Alias:# I_MPI_ADJUST_BCAST_NODE_SHUMILIN_SEGSIZE @Default:# -1
	MPIR_CVAR_BCAST_NODE_SHM_GATHER_TYPE	@Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_TYPE @Default:# -1
	MPIR_CVAR_BCAST_NODE_SHM_GATHER_RADIX	@Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_GATHER_RADIX @Default:# -1
	MPIR_CVAR_BCAST_NODE_SHM_RELEASE_TYPE	@Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_TYPE @Default:# -1
	MPIR_CVAR_BCAST_NODE_SHM_RELEASE_RADIX	@Alias:# I_MPI_ADJUST_BCAST_NODE_SHM_RELEASE_RADIX @Default:# -1
	MPIR_CVAR_BCAST_NETWORK_SHUMILIN_SEGSIZE	@Alias:# I_MPI_ADJUST_BCAST_NETWORK_SHUMILIN_SEGSIZE @Default:# -1
	MPIR_CVAR_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH	@Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_MEMCPY_ARCH @Default:# -1
	MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NUM	@Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NUM
	MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_SIZE	@Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_SIZE
	MPIR_CVAR_BCAST_NODE_NUMA_AWARE_RECV_NONTEMPORAL_THRESHOLD	@Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_NONTEMPORAL_THRESHOLD
	MPIR_CVAR_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE	@Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_FRAME_NONTEMPORAL_SIZE
	MPIR_CVAR_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD	@Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_TINY_MESSAGE_SIZE_THRESHOLD
	MPIR_CVAR_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD	@Alias:# I_MPI_ADJUST_BCAST_NODE_NUMA_AWARE_SHM_HEAP_MESSAGE_SIZE_THRESHOLD
	MPIR_CVAR_USE_EXSCAN            	Control selection of MPI_Exscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_EXSCAN @Default:# -1
	MPIR_CVAR_USE_EXSCAN_LIST       	@Alias:# I_MPI_ADJUST_EXSCAN_LIST @Default:# -1
	MPIR_CVAR_USE_EXSCAN_COMPOSITION	@Alias:# I_MPI_ADJUST_EXSCAN_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_EXSCAN_NETWORK    	@Alias:# I_MPI_ADJUST_EXSCAN_NETWORK @Default:# -1
	MPIR_CVAR_USE_EXSCAN_NODE       	@Alias:# I_MPI_ADJUST_EXSCAN_NODE @Default:# -1
	MPIR_CVAR_USE_GATHER            	Control selection of MPI_Gather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_GATHER @Default:# -1
	MPIR_CVAR_USE_GATHER_LIST       	@Alias:# I_MPI_ADJUST_GATHER_LIST @Default:# -1
	MPIR_CVAR_USE_GATHER_COMPOSITION	@Alias:# I_MPI_ADJUST_GATHER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_GATHER_NETWORK    	@Alias:# I_MPI_ADJUST_GATHER_NETWORK @Default:# -1
	MPIR_CVAR_USE_GATHER_NODE       	@Alias:# I_MPI_ADJUST_GATHER_NODE @Default:# -1
	MPIR_CVAR_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE	@Alias:# I_MPI_ADJUST_GATHER_NODE_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
	MPIR_CVAR_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE	@Alias:# I_MPI_ADJUST_GATHER_NETWORK_BINOMIAL_SEGMENTED_SEGSIZE @Default:# -1
	MPIR_CVAR_USE_GATHERV           	Control selection of MPI_Gatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_GATHERV @Default:# -1
	MPIR_CVAR_USE_GATHERV_LIST      	@Alias:# I_MPI_ADJUST_GATHERV_LIST @Default:# -1
	MPIR_CVAR_USE_GATHERV_COMPOSITION	@Alias:# I_MPI_ADJUST_GATHERV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_GATHERV_NETWORK   	@Alias:# I_MPI_ADJUST_GATHERV_NETWORK @Default:# -1
	MPIR_CVAR_USE_GATHERV_NODE      	@Alias:# I_MPI_ADJUST_GATHERV_NODE @Default:# -1
	MPIR_CVAR_GATHERV_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_GATHERV_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_GATHERV_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_GATHERV_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER    	Control selection of MPI_Reduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_LIST	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_LIST @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_COMPOSITION	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_NETWORK	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NETWORK @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_NODE	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_NODE @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK	Control selection of MPI_Reduce_scatter_block algorithm presets. Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_LIST	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_LIST @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_COMPOSITION	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NETWORK	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
	MPIR_CVAR_USE_REDUCE_SCATTER_BLOCK_NODE	@Alias:# I_MPI_ADJUST_REDUCE_SCATTER_BLOCK_NODE @Default:# -1
	MPIR_CVAR_USE_REDUCE            	Control selection of MPI_Reduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-13 @Alias:# I_MPI_ADJUST_REDUCE @Default:# -1
	MPIR_CVAR_USE_REDUCE_LIST       	@Alias:# I_MPI_ADJUST_REDUCE_LIST @Default:# -1
	MPIR_CVAR_USE_REDUCE_COMPOSITION	@Alias:# I_MPI_ADJUST_REDUCE_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_REDUCE_NETWORK    	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK @Default:# -1
	MPIR_CVAR_USE_REDUCE_NODE       	@Alias:# I_MPI_ADJUST_REDUCE_NODE @Default:# -1
	MPIR_CVAR_REDUCE_NETWORK_KARY_RADIX	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_RADIX @Default:# -1
	MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_REDUCE_NETWORK_KARY_NBUFFERS	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_NBUFFERS @Default:# -1
	MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_NBUFFERS	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_NBUFFERS @Default:# -1
	MPIR_CVAR_REDUCE_NETWORK_KARY_SEGSIZE	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KARY_SEGSIZE @Default:# -1
	MPIR_CVAR_REDUCE_NETWORK_KNOMIAL_SEGSIZE	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK_KNOMIAL_SEGSIZE @Default:# -1
	MPIR_CVAR_REDUCE_NETWORK_RING_SEGSIZE	@Alias:# I_MPI_ADJUST_REDUCE_NETWORK_RING_SEGSIZE @Default:# -1
	MPIR_CVAR_REDUCE_NODE_KARY_RADIX	@Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_RADIX @Default:# -1
	MPIR_CVAR_REDUCE_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_REDUCE_NODE_KARY_NBUFFERS	@Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_NBUFFERS @Default:# -1
	MPIR_CVAR_REDUCE_NODE_KNOMIAL_NBUFFERS	@Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_NBUFFERS @Default:# -1
	MPIR_CVAR_REDUCE_NODE_KARY_SEGSIZE	@Alias:# I_MPI_ADJUST_REDUCE_NODE_KARY_SEGSIZE @Default:# -1
	MPIR_CVAR_REDUCE_NODE_KNOMIAL_SEGSIZE	@Alias:# I_MPI_ADJUST_REDUCE_NODE_KNOMIAL_SEGSIZE @Default:# -1
	MPIR_CVAR_REDUCE_NODE_RING_SEGSIZE	@Alias:# I_MPI_ADJUST_REDUCE_NODE_RING_SEGSIZE @Default:# -1
	MPIR_CVAR_REDUCE_NODE_SHM_GATHER_TYPE	@Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_TYPE @Default:# -1
	MPIR_CVAR_REDUCE_NODE_SHM_GATHER_RADIX	@Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_GATHER_RADIX @Default:# -1
	MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_TYPE	@Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_TYPE @Default:# -1
	MPIR_CVAR_REDUCE_NODE_SHM_RELEASE_RADIX	@Alias:# I_MPI_ADJUST_REDUCE_NODE_SHM_RELEASE_RADIX @Default:# -1
	MPIR_CVAR_USE_SCAN              	Control selection of MPI_Scan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_SCAN @Default:# -1
	MPIR_CVAR_USE_SCAN_LIST         	@Alias:# I_MPI_ADJUST_SCAN_LIST @Default:# -1
	MPIR_CVAR_USE_SCAN_COMPOSITION  	@Alias:# I_MPI_ADJUST_SCAN_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_SCAN_NETWORK      	@Alias:# I_MPI_ADJUST_SCAN_NETWORK @Default:# -1
	MPIR_CVAR_USE_SCAN_NODE         	@Alias:# I_MPI_ADJUST_SCAN_NODE @Default:# -1
	MPIR_CVAR_USE_SCATTER           	Control selection of MPI_Scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_SCATTER @Default:# -1
	MPIR_CVAR_USE_SCATTER_LIST      	@Alias:# I_MPI_ADJUST_SCATTER_LIST @Default:# -1
	MPIR_CVAR_USE_SCATTER_COMPOSITION	@Alias:# I_MPI_ADJUST_SCATTER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_SCATTER_NETWORK   	@Alias:# I_MPI_ADJUST_SCATTER_NETWORK @Default:# -1
	MPIR_CVAR_USE_SCATTER_NODE      	@Alias:# I_MPI_ADJUST_SCATTER_NODE @Default:# -1
	MPIR_CVAR_USE_SCATTERV          	Control selection of MPI_Scatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_SCATTERV @Default:# -1
	MPIR_CVAR_USE_SCATTERV_LIST     	@Alias:# I_MPI_ADJUST_SCATTERV_LIST @Default:# -1
	MPIR_CVAR_USE_SCATTERV_COMPOSITION	@Alias:# I_MPI_ADJUST_SCATTERV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_SCATTERV_NETWORK  	@Alias:# I_MPI_ADJUST_SCATTERV_NETWORK @Default:# -1
	MPIR_CVAR_USE_SCATTERV_NODE     	@Alias:# I_MPI_ADJUST_SCATTERV_NODE @Default:# -1
	MPIR_CVAR_USE_IALLREDUCE        	Control selection of MPI_Iallreduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-9 @Alias:# I_MPI_ADJUST_IALLREDUCE @Default:# -1
	MPIR_CVAR_USE_IALLREDUCE_LIST   	@Alias:# I_MPI_ADJUST_IALLREDUCE_LIST @Default:# -1
	MPIR_CVAR_USE_IALLREDUCE_COMPOSITION	@Alias:# I_MPI_ADJUST_IALLREDUCE_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IALLREDUCE_NETWORK	@Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK @Default:# -1
	MPIR_CVAR_USE_IALLREDUCE_NODE   	@Alias:# I_MPI_ADJUST_IALLREDUCE_NODE @Default:# -1
	MPIR_CVAR_IALLREDUCE_KNOMIAL_REDUCE_RADIX	@Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_REDUCE_RADIX @Default:# -1
	MPIR_CVAR_IALLREDUCE_KNOMIAL_BCAST_RADIX	@Alias:# I_MPI_ADJUST_IALLREDUCE_KNOMIAL_BCAST_RADIX @Default:# -1
	MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX	@Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_REDUCE_RADIX @Default:# -1
	MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX	@Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_REDUCE_RADIX @Default:# -1
	MPIR_CVAR_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX	@Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_KNOMIAL_BCAST_RADIX @Default:# -1
	MPIR_CVAR_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX	@Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_KNOMIAL_BCAST_RADIX @Default:# -1
	MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_DO_GATHER	@Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_DO_GATHER @Default:# -1
	MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER	@Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_DO_GATHER @Default:# -1
	MPIR_CVAR_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER	@Alias:# I_MPI_ADJUST_IALLREDUCE_NODE_NREDUCE_NB_ALLGATHER @Default:# -1
	MPIR_CVAR_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER	@Alias:# I_MPI_ADJUST_IALLREDUCE_NETWORK_NREDUCE_NB_ALLGATHER @Default:# -1
	MPIR_CVAR_USE_IBCAST            	Control selection of MPI_Ibcast algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IBCAST @Default:# -1
	MPIR_CVAR_USE_IBCAST_LIST       	@Alias:# I_MPI_ADJUST_IBCAST_LIST @Default:# -1
	MPIR_CVAR_USE_IBCAST_COMPOSITION	@Alias:# I_MPI_ADJUST_IBCAST_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IBCAST_NETWORK    	@Alias:# I_MPI_ADJUST_IBCAST_NETWORK @Default:# -1
	MPIR_CVAR_USE_IBCAST_NODE       	@Alias:# I_MPI_ADJUST_IBCAST_NODE @Default:# -1
	MPIR_CVAR_IBCAST_KNOMIAL_RADIX  	@Alias:# I_MPI_ADJUST_IBCAST_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_IBCAST_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IBCAST_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_IBCAST_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IBCAST_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_USE_IREDUCE           	Control selection of MPI_Ireduce algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-4 @Alias:# I_MPI_ADJUST_IREDUCE @Default:# -1
	MPIR_CVAR_USE_IREDUCE_LIST      	@Alias:# I_MPI_ADJUST_IREDUCE_LIST @Default:# -1
	MPIR_CVAR_USE_IREDUCE_COMPOSITION	@Alias:# I_MPI_ADJUST_IREDUCE_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IREDUCE_NETWORK   	@Alias:# I_MPI_ADJUST_IREDUCE_NETWORK @Default:# -1
	MPIR_CVAR_USE_IREDUCE_NODE      	@Alias:# I_MPI_ADJUST_IREDUCE_NODE @Default:# -1
	MPIR_CVAR_IREDUCE_KNOMIAL_RADIX 	@Alias:# I_MPI_ADJUST_IREDUCE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_IREDUCE_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IREDUCE_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_IREDUCE_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IREDUCE_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_USE_IGATHER           	Control selection of MPI_Igather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IGATHER @Default:# -1
	MPIR_CVAR_USE_IGATHER_LIST      	@Alias:# I_MPI_ADJUST_IGATHER_LIST @Default:# -1
	MPIR_CVAR_USE_IGATHER_COMPOSITION	@Alias:# I_MPI_ADJUST_IGATHER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IGATHER_NETWORK   	@Alias:# I_MPI_ADJUST_IGATHER_NETWORK @Default:# -1
	MPIR_CVAR_USE_IGATHER_NODE      	@Alias:# I_MPI_ADJUST_IGATHER_NODE @Default:# -1
	MPIR_CVAR_IGATHER_KNOMIAL_RADIX 	@Alias:# I_MPI_ADJUST_IGATHER_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_IGATHER_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IGATHER_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_IGATHER_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IGATHER_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_USE_IALLGATHER        	Control selection of MPI_Iallgather algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHER @Default:# -1
	MPIR_CVAR_USE_IALLGATHER_LIST   	@Alias:# I_MPI_ADJUST_IALLGATHER_LIST @Default:# -1
	MPIR_CVAR_USE_IALLGATHER_COMPOSITION	@Alias:# I_MPI_ADJUST_IALLGATHER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IALLGATHER_NETWORK	@Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK @Default:# -1
	MPIR_CVAR_USE_IALLGATHER_NODE   	@Alias:# I_MPI_ADJUST_IALLGATHER_NODE @Default:# -1
	MPIR_CVAR_IALLGATHER_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IALLGATHER_NETWORK_KNOMIAL_RADIX
	MPIR_CVAR_IALLGATHER_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_IALLGATHER_NODE_KNOMIAL_RADIX
	MPIR_CVAR_USE_IALLTOALL         	Control selection of MPI_Ialltoall algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-7 @Alias:# I_MPI_ADJUST_IALLTOALL @Default:# -1
	MPIR_CVAR_USE_IALLTOALL_LIST    	@Alias:# I_MPI_ADJUST_IALLTOALL_LIST @Default:# -1
	MPIR_CVAR_USE_IALLTOALL_COMPOSITION	@Alias:# I_MPI_ADJUST_IALLTOALL_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IALLTOALL_NETWORK 	@Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK @Default:# -1
	MPIR_CVAR_USE_IALLTOALL_NODE    	@Alias:# I_MPI_ADJUST_IALLTOALL_NODE @Default:# -1
	MPIR_CVAR_IALLTOALL_PERMUTED_SENDRECV_THROTTLE	@Alias:# I_MPI_ADJUST_IALLTOALL_PERMUTED_SENDRECV_THROTTLE @Default:# -1
	MPIR_CVAR_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE	@Alias:# I_MPI_ADJUST_IALLTOALL_NETWORK_PERMUTED_SENDRECV_THROTTLE @Default:# -1
	MPIR_CVAR_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE	@Alias:# I_MPI_ADJUST_IALLTOALL_NODE_PERMUTED_SENDRECV_THROTTLE @Default:# -1
	MPIR_CVAR_USE_IALLTOALLV        	Control selection of MPI_Ialltoallv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-6 @Alias:# I_MPI_ADJUST_IALLTOALLV @Default:# -1
	MPIR_CVAR_USE_IALLTOALLV_LIST   	@Alias:# I_MPI_ADJUST_IALLTOALLV_LIST @Default:# -1
	MPIR_CVAR_USE_IALLTOALLV_COMPOSITION	@Alias:# I_MPI_ADJUST_IALLTOALLV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IALLTOALLV_NETWORK	@Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK @Default:# -1
	MPIR_CVAR_USE_IALLTOALLV_NODE   	@Alias:# I_MPI_ADJUST_IALLTOALLV_NODE @Default:# -1
	MPIR_CVAR_IALLTOALLV_BLOCKED_THROTTLE	@Alias:# I_MPI_ADJUST_IALLTOALLV_BLOCKED_THROTTLE @Default:# -1
	MPIR_CVAR_IALLTOALLV_NETWORK_BLOCKED_THROTTLE	@Alias:# I_MPI_ADJUST_IALLTOALLV_NETWORK_BLOCKED_THROTTLE @Default:# -1
	MPIR_CVAR_USE_IALLTOALLW        	Control selection of MPI_Ialltoallw algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IALLTOALLW @Default:# -1
	MPIR_CVAR_USE_IALLTOALLW_LIST   	@Alias:# I_MPI_ADJUST_IALLTOALLW_LIST @Default:# -1
	MPIR_CVAR_USE_IALLTOALLW_COMPOSITION	@Alias:# I_MPI_ADJUST_IALLTOALLW_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IALLTOALLW_NETWORK	@Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK @Default:# -1
	MPIR_CVAR_USE_IALLTOALLW_NODE   	@Alias:# I_MPI_ADJUST_IALLTOALLW_NODE @Default:# -1
	MPIR_CVAR_IALLTOALLW_BLOCKED_THROTTLE	@Alias:# I_MPI_ADJUST_IALLTOALLW_BLOCKED_THROTTLE @Default:# -1
	MPIR_CVAR_IALLTOALLW_NETWORK_BLOCKED_THROTTLE	@Alias:# I_MPI_ADJUST_IALLTOALLW_NETWORK_BLOCKED_THROTTLE @Default:# -1
	MPIR_CVAR_USE_IGATHERV          	Control selection of MPI_Igatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IGATHERV @Default:# -1
	MPIR_CVAR_USE_IGATHERV_LIST     	@Alias:# I_MPI_ADJUST_IGATHERV_LIST @Default:# -1
	MPIR_CVAR_USE_IGATHERV_COMPOSITION	@Alias:# I_MPI_ADJUST_IGATHERV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IGATHERV_NETWORK  	@Alias:# I_MPI_ADJUST_IGATHERV_NETWORK @Default:# -1
	MPIR_CVAR_USE_IGATHERV_NODE     	@Alias:# I_MPI_ADJUST_IGATHERV_NODE @Default:# -1
	MPIR_CVAR_USE_ISCATTERV         	Control selection of MPI_Iscatterv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCATTERV @Default:# -1
	MPIR_CVAR_USE_ISCATTERV_LIST    	@Alias:# I_MPI_ADJUST_ISCATTERV_LIST @Default:# -1
	MPIR_CVAR_USE_ISCATTERV_COMPOSITION	@Alias:# I_MPI_ADJUST_ISCATTERV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ISCATTERV_NETWORK 	@Alias:# I_MPI_ADJUST_ISCATTERV_NETWORK @Default:# -1
	MPIR_CVAR_USE_ISCATTERV_NODE    	@Alias:# I_MPI_ADJUST_ISCATTERV_NODE @Default:# -1
	MPIR_CVAR_USE_IBARRIER          	Control selection of MPI_Ibarrier algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_IBARRIER @Default:# -1
	MPIR_CVAR_USE_IBARRIER_LIST     	@Alias:# I_MPI_ADJUST_IBARRIER_LIST @Default:# -1
	MPIR_CVAR_USE_IBARRIER_COMPOSITION	@Alias:# I_MPI_ADJUST_IBARRIER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IBARRIER_NETWORK  	@Alias:# I_MPI_ADJUST_IBARRIER_NETWORK @Default:# -1
	MPIR_CVAR_USE_IBARRIER_NODE     	@Alias:# I_MPI_ADJUST_IBARRIER_NODE @Default:# -1
	MPIR_CVAR_USE_ISCATTER          	Control selection of MPI_Iscatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-2 @Alias:# I_MPI_ADJUST_ISCATTER @Default:# -1
	MPIR_CVAR_USE_ISCATTER_LIST     	@Alias:# I_MPI_ADJUST_ISCATTER_LIST @Default:# -1
	MPIR_CVAR_USE_ISCATTER_COMPOSITION	@Alias:# I_MPI_ADJUST_ISCATTER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ISCATTER_NETWORK  	@Alias:# I_MPI_ADJUST_ISCATTER_NETWORK @Default:# -1
	MPIR_CVAR_USE_ISCATTER_NODE     	@Alias:# I_MPI_ADJUST_ISCATTER_NODE @Default:# -1
	MPIR_CVAR_ISCATTER_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_ISCATTER_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_ISCATTER_NETWORK_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_ISCATTER_NETWORK_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_ISCATTER_NODE_KNOMIAL_RADIX	@Alias:# I_MPI_ADJUST_ISCATTER_NODE_KNOMIAL_RADIX @Default:# -1
	MPIR_CVAR_USE_IALLGATHERV       	Control selection of MPI_Iallgatherv algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-5 @Alias:# I_MPI_ADJUST_IALLGATHERV @Default:# -1
	MPIR_CVAR_USE_IALLGATHERV_LIST  	@Alias:# I_MPI_ADJUST_IALLGATHERV_LIST @Default:# -1
	MPIR_CVAR_USE_IALLGATHERV_COMPOSITION	@Alias:# I_MPI_ADJUST_IALLGATHERV_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IALLGATHERV_NETWORK	@Alias:# I_MPI_ADJUST_IALLGATHERV_NETWORK @Default:# -1
	MPIR_CVAR_USE_IALLGATHERV_NODE  	@Alias:# I_MPI_ADJUST_IALLGATHERV_NODE @Default:# -1
	MPIR_CVAR_USE_IEXSCAN           	Control selection of MPI_Iexscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_IEXSCAN @Default:# -1
	MPIR_CVAR_USE_IEXSCAN_LIST      	@Alias:# I_MPI_ADJUST_IEXSCAN_LIST @Default:# -1
	MPIR_CVAR_USE_IEXSCAN_COMPOSITION	@Alias:# I_MPI_ADJUST_IEXSCAN_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IEXSCAN_NETWORK   	@Alias:# I_MPI_ADJUST_IEXSCAN_NETWORK @Default:# -1
	MPIR_CVAR_USE_IEXSCAN_NODE      	@Alias:# I_MPI_ADJUST_IEXSCAN_NODE @Default:# -1
	MPIR_CVAR_USE_ISCAN             	Control selection of MPI_Iscan algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-1 @Alias:# I_MPI_ADJUST_ISCAN @Default:# -1
	MPIR_CVAR_USE_ISCAN_LIST        	@Alias:# I_MPI_ADJUST_ISCAN_LIST @Default:# -1
	MPIR_CVAR_USE_ISCAN_COMPOSITION 	@Alias:# I_MPI_ADJUST_ISCAN_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_ISCAN_NETWORK     	@Alias:# I_MPI_ADJUST_ISCAN_NETWORK @Default:# -1
	MPIR_CVAR_USE_ISCAN_NODE        	@Alias:# I_MPI_ADJUST_ISCAN_NODE @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER   	Control selection of MPI_Ireduce_scatter algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_LIST	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_LIST @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_COMPOSITION	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_NETWORK	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NETWORK @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_NODE	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_NODE @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK	Control selection of MPI_Ireduce_scatter_block algorithm presets.$ Arguments$ <algid> - Algorithm identifier$ range: 0-3 @Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_LIST	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_LIST @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_COMPOSITION	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_COMPOSITION @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NETWORK	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NETWORK @Default:# -1
	MPIR_CVAR_USE_IREDUCE_SCATTER_BLOCK_NODE	@Alias:# I_MPI_ADJUST_IREDUCE_SCATTER_BLOCK_NODE @Default:# -1
	MPIR_CVAR_IMPI_SHMGR_DATASIZE   	Define the size of shared memory area available for each rank for data placement. Messages greater than this value will not be processed by SHM-based collective operation, but will be processed by point-to-point based collective operation. The value must be a multiple of 4096. @Alias:# I_MPI_COLL_SHM_THRESHOLD @Default:# 16384
	MPIR_CVAR_IMPI_SHMGR_SPINCOUNT  	@Alias:# I_MPI_COLL_SHM_PROGRESS_SPIN_COUNT
	MPIR_CVAR_INTEL_COLL_INTRANODE  	@Alias:# I_MPI_COLL_INTRANODE
	MPIR_CVAR_ENABLE_EXPERIMENTAL_ALGOS	@Alias:# I_MPI_COLL_EXPERIMENTAL
	MPIR_CVAR_IMPI_WAIT_MODE        	@Alias:# I_MPI_WAIT_MODE
	MPIR_CVAR_IMPI_THREAD_SLEEP     	@Alias:# I_MPI_THREAD_SLEEP
	MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE	For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
	MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE	For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
	MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE	The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
	MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE	the short message algorithm will be used if the send buffer size is <= this value (in bytes)
	MPIR_CVAR_ENABLE_SMP_COLLECTIVES	Enable SMP aware collective communication.
	MPIR_CVAR_ENABLE_SMP_ALLREDUCE  	Enable SMP aware allreduce.
	MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE	Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
	MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE	the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
	MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE	the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
	MPIR_CVAR_ENABLE_SMP_BARRIER    	Enable SMP aware barrier.
	MPIR_CVAR_BCAST_MIN_PROCS       	Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
	MPIR_CVAR_BCAST_SHORT_MSG_SIZE  	Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
	MPIR_CVAR_BCAST_LONG_MSG_SIZE   	Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
	MPIR_CVAR_ENABLE_SMP_BCAST      	Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
	MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE	Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
	MPIR_CVAR_GATHER_VSMALL_MSG_SIZE	use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
	MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE	use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
	MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS	Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
	MPIR_CVAR_REDUCE_SHORT_MSG_SIZE 	the short message algorithm will be used if the send buffer size is <= this value (in bytes)
	MPIR_CVAR_ENABLE_SMP_REDUCE     	Enable SMP aware reduce.
	MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE	Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
	MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE	use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
	MPIR_CVAR_USE_CPU_PLATFORM      	@Alias:# I_MPI_PLATFORM
	MPIR_CVAR_FAILURE_ON_COLL_FALLBACK	@Alias:# I_MPI_ADJUST_FAILURE_ON_COLL_FALLBACK
	MPIR_CVAR_FAILURE_ON_MATCH_FALLBACK	@Alias:# I_MPI_ADJUST_FAILURE_ON_MATCH_FALLBACK
	MPIR_CVAR_NUMERICAL_REPRODUCIBILITY	@Alias:# I_MPI_CBWR
	MPIR_CVAR_USE_TUNING_CH4        	@Alias:# I_MPI_TUNING
	MPIR_CVAR_USE_TUNING_NET        	@Alias:# I_MPI_TUNING_NETWORK
	MPIR_CVAR_USE_TUNING_SHM        	@Alias:# I_MPI_TUNING_NODE
	MPIR_CVAR_DUMP_TUNING_CH4       	@Alias:# I_MPI_TUNING_COMPOSITION_DUMP
	MPIR_CVAR_DUMP_TUNING_NET       	@Alias:# I_MPI_TUNING_NETWORK_DUMP
	MPIR_CVAR_DUMP_TUNING_SHM       	@Alias:# I_MPI_TUNING_NODE_DUMP
	MPIR_CVAR_BIN_TUNING            	@Alias:# I_MPI_TUNING_BIN
	MPIR_CVAR_BIN_DUMP_TUNING       	@Alias:# I_MPI_TUNING_BIN_DUMP
	MPIR_CVAR_TUNING_BIN_PATH       	@Alias:# I_MPI_TUNING_BIN_PATH
	MPIR_CVAR_TUNING_COMPOSITION_PPN	@Alias:# I_MPI_TUNING_COMPOSITION_PPN
	MPIR_CVAR_TUNING_NETWORK_PPN    	@Alias:# I_MPI_TUNING_NETWORK_PPN
	MPIR_CVAR_TUNING_NODE_PPN       	@Alias:# I_MPI_TUNING_NODE_PPN
	MPIR_CVAR_TUNING_COMPOSITION_COMM_HIERARCHY	@Alias:# I_MPI_TUNING_COMPOSITION_COMM_HIERARCHY
	MPIR_CVAR_TUNING_NETWORK_COMM_HIERARCHY	@Alias:# I_MPI_TUNING_NETWORK_COMM_HIERARCHY
	MPIR_CVAR_TUNING_NODE_COMM_HIERARCHY	@Alias:# I_MPI_TUNING_NODE_COMM_HIERARCHY
	MPIR_CVAR_TUNING_MODE           	@Alias:# I_MPI_TUNING_MODE
	MPIR_CVAR_TUNING_AUTO_POLICY    	@Alias:# I_MPI_TUNING_AUTO_POLICY
	MPIR_CVAR_TUNING_AUTO_COMM_LIST 	@Alias:# I_MPI_TUNING_AUTO_COMM_LIST
	MPIR_CVAR_TUNING_AUTO_COMM_USER 	@Alias:# I_MPI_TUNING_AUTO_COMM_USER
	MPIR_CVAR_TUNING_AUTO_COMM_DEFAULT	@Alias:# I_MPI_TUNING_AUTO_COMM_DEFAULT
	MPIR_CVAR_TUNING_AUTO_ITER_NUM  	@Alias:# I_MPI_TUNING_AUTO_ITER_NUM
	MPIR_CVAR_TUNING_AUTO_ITER_POLICY	@Alias:# I_MPI_TUNING_AUTO_ITER_POLICY
	MPIR_CVAR_TUNING_AUTO_ITER_POLICY_THRESHOLD	@Alias:# I_MPI_TUNING_AUTO_ITER_POLICY_THRESHOLD
	MPIR_CVAR_TUNING_AUTO_SYNC      	@Alias:# I_MPI_TUNING_AUTO_SYNC
	MPIR_CVAR_TUNING_AUTO_STORAGE_SIZE	@Alias:# I_MPI_TUNING_AUTO_STORAGE_SIZE
	MPIR_CVAR_TUNING_AUTO_WARMUP_ITER_NUM	@Alias:# I_MPI_TUNING_AUTO_WARMUP_ITER_NUM
	MPIR_CVAR_TUNING_AUTO_SMART     	@Alias:# I_MPI_TUNING_AUTO_SMART
	MPIR_CVAR_TUNING_COLL_LIST      	@Alias:# I_MPI_TUNING_COLL_LIST
	MPIR_CVAR_TUNING_COLL_VEC_OPS   	@Alias:# I_MPI_TUNING_COLL_VEC_OPS
	MPIR_CVAR_DEFAULT_THREAD_LEVEL  	@Alias:# I_MPI_THREAD_LEVEL
	MPIR_CVAR_THREAD_SPLIT          	Control the MPI_THREAD_SPLIT model support. @Alias:# I_MPI_THREAD_SPLIT @Default:# false
	MPIR_CVAR_THREAD_RUNTIME        	Control threading runtimes support. @Alias:# I_MPI_THREAD_RUNTIME @Default:# generic
	MPIR_CVAR_THREAD_ID_KEY         	Set the MPI info object key that is used to explicitly define the application $thread_id for a communicator. @Alias:# I_MPI_THREAD_ID_KEY @Default:# -1
	MPIR_CVAR_THREAD_MAX            	Set the maximum number of application threads per rank. @Alias:# I_MPI_THREAD_MAX @Default:# -1
	MPIR_CVAR_ASYNC_PROGRESS        	Enables asynchronous progress threads. @Alias:# I_MPI_ASYNC_PROGRESS @Default:# false
	MPIR_CVAR_CH4_MAX_PROGRESS_THREADS	Specifies the maximum number of progress threads. @Alias:# I_MPI_ASYNC_PROGRESS_THREADS @Default:# 1
	MPIR_CVAR_CH4_PROGRESS_THREAD_AFFINITY	Specifies affinity for all progress threads of local processes. @Alias:# I_MPI_ASYNC_PROGRESS_PIN @Default:# not defined
	MPIR_CVAR_EP_ID_KEY             	Set the MPI info object key that is used to explicitly define the progress $thread_id for a communicator. @Alias:# I_MPI_ASYNC_PROGRESS_ID_KEY @Default:# thread_id
	MPIR_CVAR_THREAD_MODE           	@Alias:# I_MPI_THREAD_MODE
	MPIR_CVAR_THREAD_LOCK_LEVEL     	@Alias:# I_MPI_THREAD_LOCK_LEVEL
	MPIR_CVAR_CH4_OFI_MAX_VCIS      	@Alias:# I_MPI_THREAD_EP_MAX
	MPIR_CVAR_OFFLOAD_TOPOLIB       	@Alias:# I_MPI_OFFLOAD_TOPOLIB
	MPIR_CVAR_OFFLOAD_INFO_SET_NTILES	@Alias:# I_MPI_OFFLOAD_INFO_SET_NTILES
	MPIR_CVAR_OFFLOAD_INFO_SET_NGPUS	@Alias:# I_MPI_OFFLOAD_INFO_SET_NGPUS
	MPIR_CVAR_OFFLOAD_INFO_SET_NNUMANODES	@Alias:# I_MPI_OFFLOAD_INFO_SET_NNUMANODES
	MPIR_CVAR_OFFLOAD_INFO_SET_GPU_ID	@Alias:# I_MPI_OFFLOAD_INFO_SET_GPU_ID
	MPIR_CVAR_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS	@Alias:# I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_GPUS
	MPIR_CVAR_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS	@Alias:# I_MPI_OFFLOAD_INFO_SET_NUMANODE_ID_FOR_RANKS
	MPIR_CVAR_OFFLOAD_INFO_SET_VENDOR_ID	@Alias:# I_MPI_OFFLOAD_INFO_SET_VENDOR_ID
	MPIR_CVAR_OFFLOAD_DOMAIN_SIZE   	@Alias:# I_MPI_OFFLOAD_DOMAIN_SIZE
	MPIR_CVAR_OFFLOAD_CELL          	@Alias:# I_MPI_OFFLOAD_CELL
	MPIR_CVAR_OFFLOAD_DEVICES       	@Alias:# I_MPI_OFFLOAD_DEVICES
	MPIR_CVAR_OFFLOAD_DEVICE_LIST   	@Alias:# I_MPI_OFFLOAD_DEVICE_LIST
	MPIR_CVAR_OFFLOAD_DOMAIN        	@Alias:# I_MPI_OFFLOAD_DOMAIN
	MPIR_CVAR_OFFLOAD_LEVEL_ZERO_LIBRARY	@Alias:# I_MPI_OFFLOAD_LEVEL_ZERO_LIBRARY
	MPIR_CVAR_INTEL_DEBUG           	Print out debugging information when an MPI program starts running.$ Syntax$ I_MPI_DEBUG=<level>$ Arguments$ <level> - indicate level of debug information provided @Alias:# I_MPI_DEBUG @Default:# 0
	MPIR_CVAR_DEBUG_VERSION         	Print Intel MPI version. @Alias:# I_MPI_PRINT_VERSION @Default:# 0
	MPIR_CVAR_ERROR_CHECKING        	@Alias:# I_MPI_ERROR_CHECKING
	MPIR_CVAR_MULTI_INIT            	@Alias:# I_MPI_MULTI_INIT
	MPIR_CVAR_REMOVED_VAR_WARNING   	Print out a warning if a removed environment variable is set. @Alias:# I_MPI_REMOVED_VAR_WARNING @Default:# 1
	MPIR_CVAR_VAR_CHECK_SPELLING    	Print out a warning if an unknown environment variable is set. @Alias:# I_MPI_VAR_CHECK_SPELLING @Default:# 1
	MPIR_CVAR_INTEL_MPI_COMPATIBILITY	Select the runtime compatibility mode.$ Syntax$ I_MPI_COMPATIBILITY=<value>$ Arguments$ <value> - Define compatibility mode$ -----------------------------------------------------------------------$ <not defined> - The MPI-3.1 standard compatibility$ <3> - The Intel® MPI Library 3.x compatible mode$ <4> - The Intel® MPI Library 4.x compatible mode$ <5> - The Intel® MPI Library 5.x compatible mode$ ----------------------------------------------------------------------- @Alias:# I_MPI_COMPATIBILITY @Default:# 5
	MPIR_CVAR_IMPI_PROGRESS_SPIN_COUNT	@Alias:# I_MPI_SPIN_COUNT
	MPIR_CVAR_IMPI_PROGRESS_PAUSE_COUNT	@Alias:# I_MPI_PAUSE_COUNT
	MPIR_CVAR_IMPI_THREAD_YIELD     	@Alias:# I_MPI_THREAD_YIELD
	MPIR_CVAR_SILENT_ABORT          	Do not print abort warning message @Alias:# I_MPI_SILENT_ABORT
	MPIR_CVAR_JOB_IDLE_TIMEOUT      	Abort job if idle time is larger than the threshold in seconds. @Alias:# I_MPI_JOB_IDLE_TIMEOUT
	MPIR_CVAR_PMI_VALUE_LENGTH_MAX  	Set PMI buffer length as minimum of variable value and PMI_KVS_Get_value_length_max(). @Alias:# I_MPI_PMI_VALUE_LENGTH_MAX
	MPIR_CVAR_PMI_LIBRARY           	Specify the name to third party implementation of the PMI library. @Alias:# I_MPI_PMI_LIBRARY
	MPIR_CVAR_PMI                   	Select PMI version. Choices: auto, pmi1, pmi2, pmix.$ By default pmi version will be chosen automatically. @Alias:# I_MPI_PMI
	MPIR_CVAR_NODEMAP_ALGORITHM     	Select algorithm for nodemap creation. Choices: pmi_process_mapping, slurm, pmi_alltoall, auto. @Alias:# I_MPI_NODEMAP_ALGORITHM
	MPIR_CVAR_ASYNC_REDUCE          	@Alias:# I_MPI_ASYNC_REDUCE
	MPIR_CVAR_CH4_MAX_REDUCE_THREADS	@Alias:# I_MPI_ASYNC_REDUCE_THREADS
	MPIR_CVAR_ASYNC_REDUCE_COUNT_THRESHOLD	@Alias:# I_MPI_ASYNC_REDUCE_COUNT_THRESHOLD
	MPIR_CVAR_CH4_REDUCE_THREAD_AFFINITY	@Alias:# I_MPI_ASYNC_REDUCE_PIN
